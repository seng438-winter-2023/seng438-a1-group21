>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: Group Number 21 |
|-----------------|
| Tony Tan |   
| Evan Wong |   
| Johnny Yuen |   
| Bryant Zhang |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself	1](#_Toc439194683)

# Introduction

The premise of this lab exercise is to give students an introduction and to understand the process of software testing. This is done through two provided versions of a Java ATM application, version 1.0 and version 1.1. After exploring the application, manual scripted testing, exploratory testing, and regression testing were performed following the test case. The lab also introduced students to Backlog, an application for teams to be organised and track progression of bugs.
Prior to this lab, each lab member had some experience of exploratory testing (realistic scenarios performed with no procedures) and scripted testing (structured testing), and knowledge from lecture material. However, the team has little experience in formal documentation of testing.


# High-level description of the exploratory testing plan

The first step of our exploratory testing plan was to familiarise ourselves with the ATM application in order to identify the steps and the functions we would like to test. We decided that we would first test the most common paths a customer would go through when using the application. We planned to test the most common paths since those functions would be the most used and therefore the most important and have the highest priority in terms of bug fixing. As such, we first tested the two given ATM cards and PINs because that would be the first interaction every user has with the machine and there would be security concerns if bugs were to be found in this function. We also tested the valid ATM cards with incorrect PINs and invalid ATM cards. Next, we tested the withdrawal, deposit, transfer and balance inquiry functions of the ATM. During the testing of these functions, we focused on verifying if the transaction successfully occurred and if the balance of the account matches the transaction. For example, if a deposit was made then the account balance should be updated with the addition of the deposit. 


# Comparison of exploratory and manual functional testing

Based on our testing experience in this assignment, exploratory and manual functional testing both came with their strong points and weak points. With exploratory testing you have the benefit of having a loose plan, allowing the freedom to test an application with a tester's skills and experience. It also allows for very in-depth testing of different components in specific scenarios. However it does come with tradeoffs as with exploratory testing you often come across bugs which have difficulty being replicated, or having to learn a systems tools and the best way to use them in order for effective bug testing. Exploratory testing allows you to be more effective in a more agile development space, and efficient when it comes to finding bugs in isolated components of an application. While exploratory testing in a more waterfall environment within large system spaces will be less efficient and effective.
	Manual scripted functional testing benefits from allowing testers to be able to test a system even when lacking knowledge of an application's features. This type of testing also allows for bugs to be easily reproduced, as controlling the tests is much easier in scripted than exploratory testing. Scripted testing allows for a system's specifications and goals to be met within specific test cases created for an application. The tradeoffs of scripted manual testing though is that a high level of documentation is required when creating test cases for your system, which means a long planning phase instead of jumping right into tests. Another tradeoff would be chances of smaller bugs being prevalent in certain components, because of incomplete test cases. Scripted testing is effective and efficient as it ensures that the general functionality of a component has been tested and passed within a certain timeline because a team has clear documentation to stick to. With scripted testing there are detailed steps and test cases to make sure time is used efficiently.

# Notes and discussion of the peer reviews of defect reports

When testing for bugs in these lab exercises, it was difficult for another team member to produce the same bug, without following the exact same procedure. In the team's experience, although the application was simple to work with and understand, one team member's experience interacting with the application and bug can be ambiguous to another. Therefore, it was important to record the processes and communicate them. During testing, each group tested specific components during the exploratory testing phase, which allowed for large coverage and small bugs to be found.


# How the pair testing was managed and team work/effort was divided 

How we managed the pair testing portion of our assignment and divided the work needed to be done was first split into two teams of two members each. Afterwards one member in each team conducted the testing in the exploratory phase, while the other reviewed and analysed the resulting tests. Each pair of two switched their testers and reviewers for each phase (exploratory, manual, and regression) back and forth until all of the testing was completed. Subsequently following the testing, both pairs wrote a short and concise report of their findings and combined them into a final lab report.


# Difficulties encountered, challenges overcome, and lessons learned

Some difficulties we found when testing were inexperience with the system and the bug reporting with Jira. In the beginning, we had difficulty navigating through the system and recognizing bugs since we were unsure if that was the intended behaviour. When we did find a defect, we were uncertain about all of the information required in our bug report. In the beginning we had trouble organising as a team as Jira was new to all of us. 
Another challenge we overcame was learning how to utilise Jira to report bugs and defects in a group environment. In order to effectively log bugs as a team, we utilised Jira to provide insight on what the bug was, how to replicate it, and whether it is resolved in a future version. As we became more experienced with Jira our difficulty organising disappeared.
Lessons we learnt from this lab were good communication skills, pair programming skills, attention to detail skills, and the ability to closely follow procedures. We also learnt lessons on the benefits and scenarios of when to use each type of testing. Additionally, we gained valuable, hands-on experience using an industry-standard defect tracking tool, Atlassian Jira. 

# Comments/feedback on the lab and lab document itself

The lab exercise was interesting and educational. It provided a good learning experience to an introduction of exploratory and scripted testing, and their differences. It gives an experience on Backlog, allowing teams to be organised and track bugs. The lab overall was clear on instruction, and provided students with knowledge on software testing.
